{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "# from scipy.sparse import csr_matrix # Non usata se A è densa\n",
    "import time\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def projection_simplex(v, z=1.0):\n",
    "    \"\"\"Project vector v onto the simplex (sum(x)=z, x>=0).\"\"\"\n",
    "    n = len(v)\n",
    "    if n == 0: return np.array([])\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    \n",
    "    # Trova rho: l'indice più grande tale che u[rho-1] - (cssv[rho-1] - z) / rho > 0\n",
    "    # (Usando indicizzazione 0-based per rho_idx, quindi rho = rho_idx + 1)\n",
    "    rho_idx = -1\n",
    "    for i in range(n - 1, -1, -1): # Itera da n-1 giù fino a 0\n",
    "        if u[i] + (z - cssv[i]) / (i + 1) > 0: # cssv[i] è sum_{j=0 to i} u[j]\n",
    "            rho_idx = i\n",
    "            break\n",
    "    \n",
    "    # Se nessun rho trovato (es. tutti gli u_i sono molto negativi e z piccolo),\n",
    "    # rho_idx rimane -1. Questo non dovrebbe accadere per z > 0 e input ragionevoli.\n",
    "    # In Duchi et al. 2008, si dimostra che rho >= 1.\n",
    "    if rho_idx == -1: # Fallback nel caso improbabile (es. z=0, tutti v<=0)\n",
    "        if z == 0: return np.zeros(n)\n",
    "        # Se z > 0, la teoria dice che rho >= 1 (cioè rho_idx >= 0)\n",
    "        # Questo fallback è per estrema robustezza o casi degeneri.\n",
    "        # Se tutti v_i sono <=0 e z>0, un vertice dovrebbe prendere tutto il peso.\n",
    "        # Per semplicità, se non troviamo rho, e z > 0, è una situazione anomala.\n",
    "        # L'implementazione originale con `ind[cond][-1]` era più concisa.\n",
    "        # Per ora, assumiamo che rho_idx venga trovato se z > 0.\n",
    "        # Se z>0 e non si trova rho, significa che tutti u_i + (z - cssv_i)/(i+1) <=0\n",
    "        # Considera un caso semplice: v = [-1, -2], z = 1. u = [-1, -2]. cssv = [-1, -3].\n",
    "        # i=1 (rho_idx=1): u[1] + (1 - cssv[1]) / 2 = -2 + (1 - (-3)) / 2 = -2 + 4/2 = 0. Non > 0.\n",
    "        # i=0 (rho_idx=0): u[0] + (1 - cssv[0]) / 1 = -1 + (1 - (-1)) / 1 = -1 + 2 = 1. > 0. rho_idx = 0.\n",
    "        # Quindi rho = 1.\n",
    "        # Se z > 0, rho_idx sarà almeno 0.\n",
    "        pass # rho_idx dovrebbe essere >=0\n",
    "\n",
    "    # theta = (sum_{i=0 to rho_idx} u[i] - z) / (rho_idx + 1)\n",
    "    theta = (cssv[rho_idx] - z) / (rho_idx + 1.0)\n",
    "    \n",
    "    proj_v = np.maximum(v - theta, 0)\n",
    "    \n",
    "    # Opzionale: normalizzazione finale per assicurare sum(x)=z a causa di errori numerici\n",
    "    current_sum = np.sum(proj_v)\n",
    "    if z > 1e-9: # Evita divisione per zero se z è ~0\n",
    "        if abs(current_sum - z) > 1e-7 * z: # Controlla differenza relativa\n",
    "            if current_sum > 1e-9: # Evita divisione per zero se current_sum è ~0\n",
    "                proj_v = proj_v * (z / current_sum)\n",
    "            else: # Se la somma è zero ma dovrebbe essere positiva, distribuisci z\n",
    "                  # Questo indica un problema o un caso degenere.\n",
    "                  # Ad es. assegna a un elemento o uniformemente (violerebbe sparsità)\n",
    "                  # Per ora, ci fidiamo dell'algoritmo sopra, ma questo è un fallback.\n",
    "                  # print(f\"Avviso: somma proiezione simplesso {current_sum} diversa da target {z}.\")\n",
    "                  pass\n",
    "    elif abs(current_sum) > 1e-7 : # se z=0, current_sum dovrebbe essere 0\n",
    "         proj_v = np.zeros(n)\n",
    "         \n",
    "    return proj_v\n",
    "\n",
    "\n",
    "def lmo(grad):\n",
    "    \"\"\"Linear Minimization Oracle for the simplex.\"\"\"\n",
    "    i = np.argmin(grad)\n",
    "    s = np.zeros_like(grad)\n",
    "    s[i] = 1.0\n",
    "    return s, i # Restituisce anche l'indice per comodità\n",
    "\n",
    "def f_l2(x, A):\n",
    "    \"\"\"L2-regularized objective function (da MASSIMIZZARE).\"\"\"\n",
    "    return x @ A @ x + 0.5 * np.dot(x, x)\n",
    "\n",
    "def grad_l2(x, A):\n",
    "    \"\"\"Gradient for L2-regularized objective (per MASSIMIZZAZIONE).\"\"\"\n",
    "    # Il gradiente di x^T A x è (A+A^T)x. Se A è simmetrica, 2Ax.\n",
    "    # Assumiamo A sia la matrice di adiacenza, quindi simmetrica.\n",
    "    return 2 * (A @ x) + x\n",
    "\n",
    "def f_l0(x, A, alpha=0.07, beta=5):\n",
    "    \"\"\"L0-regularized objective function (da MASSIMIZZARE).\"\"\"\n",
    "    # Il -len(x) nel tuo codice originale per exp_term è insolito.\n",
    "    # clustering.pdf Eq 33 è: α_2 Σ(e^(-βx_i) - 1)\n",
    "    # Quindi dovrebbe essere: alpha * np.sum(np.exp(-beta * x) - 1.0)\n",
    "    reg_term = alpha * np.sum(np.exp(-beta * np.clip(x, 0, None)) - 1.0) # Clip x>=0 per stabilità exp\n",
    "    return x @ A @ x + reg_term\n",
    "\n",
    "def grad_l0(x, A, alpha=0.07, beta=5):\n",
    "    \"\"\"Gradient for L0-regularized objective (per MASSIMIZZAZIONE).\"\"\"\n",
    "    # Gradiente di α_2 Σ(e^(-βx_i) - 1) è -alpha * beta * exp(-beta * x_i) per ogni componente\n",
    "    return 2 * (A @ x) - alpha * beta * np.exp(-beta * np.clip(x, 0, None))\n",
    "\n",
    "\n",
    "def line_search_generic(objective_func, x_curr, d_curr, A_matrix, gamma_max=1.0, reg_type_info='l2', alpha_l0=0.07, beta_l0=5):\n",
    "    \"\"\"Line search generica usando minimize_scalar. Minimizza -objective_func.\"\"\"\n",
    "    if np.linalg.norm(d_curr) < 1e-12: return 0.0\n",
    "    \n",
    "    # objective_func è la funzione da MASSIMIZZARE\n",
    "    # minimize_scalar minimizza, quindi gli passiamo -objective_func\n",
    "    def func_to_minimize(gamma_ls):\n",
    "        x_candidate = x_curr + gamma_ls * d_curr\n",
    "        # Assicurati che x_candidate rimanga non negativo se necessario per f_l0\n",
    "        if reg_type_info == 'l0':\n",
    "            x_candidate = np.maximum(x_candidate, 0) # L0 reg ha exp(-beta*x)\n",
    "        \n",
    "        val = objective_func(x_candidate, A_matrix) if reg_type_info == 'l2' else \\\n",
    "              objective_func(x_candidate, A_matrix, alpha=alpha_l0, beta=beta_l0)\n",
    "        return -val # Minimizziamo il negativo\n",
    "    \n",
    "    # Assicura che gamma_max_val sia un valore valido e non negativo.\n",
    "    if not (np.isfinite(gamma_max) and gamma_max >= 0): gamma_max = 1.0\n",
    "    if gamma_max < 1e-12: return 0.0\n",
    "\n",
    "    try:\n",
    "        res = minimize_scalar(func_to_minimize, bounds=(0, gamma_max), method='bounded')\n",
    "        gamma = res.x\n",
    "        if not np.isfinite(gamma): gamma = 0.0\n",
    "    except Exception:\n",
    "        gamma = 0.0 # Fallback\n",
    "        \n",
    "    return np.clip(gamma, 0.0, gamma_max)\n",
    "\n",
    "\n",
    "def frank_wolfe(A, reg_type='l2', max_iter=1000, tol=1e-6):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n # Inizia dal centroide del simplesso\n",
    "    \n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A) # Gradiente dell'obiettivo di MASSIMIZZAZIONE\n",
    "        grad_min = -grad_orig              # Gradiente dell'obiettivo di MINIMIZZAZIONE (-f_orig)\n",
    "        \n",
    "        s_vertex, _ = lmo(grad_min) # s = argmin <s, grad_min>\n",
    "        d = s_vertex - x\n",
    "        \n",
    "        # FW Gap per il problema di MASSIMIZZAZIONE: <grad_orig, s - x>\n",
    "        gap = grad_orig @ d \n",
    "        \n",
    "        if gap < tol:\n",
    "            # print(f\"FW iter {t}: Gap {gap:.2e} < tol. Converged.\")\n",
    "            break\n",
    "            \n",
    "        if reg_type == 'l2':\n",
    "            gamma = line_search_generic(objective_function, x, d, A, gamma_max=1.0, reg_type_info='l2')\n",
    "        else: # Per L0, usa lo step size standard di FW o line search\n",
    "            # gamma = 2.0 / (t + 2.0)\n",
    "             gamma = line_search_generic(objective_function, x, d, A, gamma_max=1.0, reg_type_info='l0')\n",
    "\n",
    "        x = x + gamma * d\n",
    "    return x\n",
    "\n",
    "def pairwise_frank_wolfe(A, reg_type='l2', max_iter=1000, tol=1e-6):\n",
    "    n = A.shape[0]\n",
    "    x0_idx = 0 \n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0 # Inizia da un vertice\n",
    "    \n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "    \n",
    "    # L'active_set tiene traccia degli indici degli atomi (vertici e_i)\n",
    "    # che hanno peso non nullo in x. x stesso contiene i pesi.\n",
    "    # active_set_indices = {x0_idx} # Non strettamente necessario se x è denso\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A)\n",
    "        grad_min = -grad_orig\n",
    "        \n",
    "        s_vertex_fw, s_index = lmo(grad_min) # Atomo FW s_t\n",
    "        \n",
    "        # FW Gap per criterio di arresto (basato su massimizzazione)\n",
    "        d_for_gap = s_vertex_fw - x\n",
    "        gap = grad_orig @ d_for_gap\n",
    "        if gap < tol:\n",
    "            # print(f\"PFW iter {t}: FW Gap {gap:.2e} < tol. Converged.\")\n",
    "            break\n",
    "            \n",
    "        # Atomo Away v_t: argmax_{v in current support of x} <grad_orig, v>\n",
    "        # Cioè, trova l'indice i nel supporto di x per cui grad_orig[i] è massimo.\n",
    "        current_support_indices = np.where(x > 1e-9)[0]\n",
    "        if len(current_support_indices) == 0:\n",
    "            # print(f\"PFW iter {t}: Supporto di x vuoto! Reset anomalo a s_index.\")\n",
    "            x.fill(0.0); x[s_index] = 1.0\n",
    "            # active_set_indices = {s_index}\n",
    "            continue\n",
    "\n",
    "        v_index = current_support_indices[np.argmax(grad_orig[current_support_indices])]\n",
    "        \n",
    "        if s_index == v_index:\n",
    "            # Passo PFW nullo, potremmo fare un passo FW standard o saltare\n",
    "            # print(f\"PFW iter {t}: s_index == v_index. Using FW-like step.\")\n",
    "            d = s_vertex_fw - x # Direzione FW\n",
    "            gamma_max_ls = 1.0\n",
    "        else:\n",
    "            v_vertex = np.zeros(n); v_vertex[v_index] = 1.0\n",
    "            d = s_vertex_fw - v_vertex # Direzione Pairwise\n",
    "            gamma_max_ls = x[v_index] # Peso alpha_v_t\n",
    "\n",
    "        gamma = line_search_generic(objective_function, x, d, A, gamma_max=gamma_max_ls, reg_type_info=reg_type)\n",
    "            \n",
    "        # Aggiorna x direttamente (poiché x contiene i pesi alpha_i per gli atomi e_i)\n",
    "        if s_index == v_index: # Logica di aggiornamento tipo FW\n",
    "             x = (1.0 - gamma) * x + gamma * s_vertex_fw\n",
    "        else: # Logica di aggiornamento Pairwise\n",
    "            x[s_index] += gamma\n",
    "            x[v_index] -= gamma\n",
    "        \n",
    "        # Pulisci pesi molto piccoli e normalizza (opzionale ma buono per stabilità)\n",
    "        x[x < 1e-10] = 0.0\n",
    "        current_sum = np.sum(x)\n",
    "        if abs(current_sum - 1.0) > 1e-7 and current_sum > 1e-9:\n",
    "            x /= current_sum\n",
    "            \n",
    "    return x\n",
    "\n",
    "def away_step_frank_wolfe(A, reg_type='l2', max_iter=1000, tol=1e-6):\n",
    "    n = A.shape[0]\n",
    "    x0_idx = 0\n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0\n",
    "\n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A)\n",
    "        grad_min = -grad_orig\n",
    "\n",
    "        s_vertex_fw, s_index = lmo(grad_min) # Atomo FW s_t\n",
    "        d_fw = s_vertex_fw - x\n",
    "        \n",
    "        # FW Gap per criterio di arresto (basato su massimizzazione)\n",
    "        gap_fw = grad_orig @ d_fw\n",
    "        if gap_fw < tol:\n",
    "            # print(f\"AFW iter {t}: FW Gap {gap_fw:.2e} < tol. Converged.\")\n",
    "            break\n",
    "\n",
    "        # Atomo Away v_t: argmax_{v in current support of x} <grad_orig, v>\n",
    "        current_support_indices = np.where(x > 1e-9)[0]\n",
    "        if len(current_support_indices) == 0:\n",
    "            # print(f\"AFW iter {t}: Supporto di x vuoto! Reset anomalo a s_index.\")\n",
    "            x.fill(0.0); x[s_index] = 1.0\n",
    "            continue\n",
    "            \n",
    "        v_index = current_support_indices[np.argmax(grad_orig[current_support_indices])]\n",
    "        v_vertex = np.zeros(n); v_vertex[v_index] = 1.0\n",
    "        d_away = x - v_vertex\n",
    "        \n",
    "        # Progresso potenziale per la MASSIMIZZAZIONE: <grad_orig, d>\n",
    "        # Scegli la direzione che massimizza <grad_orig, d>\n",
    "        potential_progress_fw = grad_orig @ d_fw\n",
    "        potential_progress_away = grad_orig @ d_away\n",
    "        \n",
    "        direction_type = \"\"\n",
    "        if potential_progress_fw >= potential_progress_away:\n",
    "            d = d_fw\n",
    "            gamma_max_ls = 1.0\n",
    "            direction_type = \"FW\"\n",
    "        else:\n",
    "            d = d_away\n",
    "            alpha_v_t = x[v_index]\n",
    "            if abs(1.0 - alpha_v_t) < 1e-9 or np.linalg.norm(d_away) < 1e-9:\n",
    "                # Se d_away è zero (x è v_vertex) o alpha_v_t è 1, gamma_max per away non è ben def.\n",
    "                # In questo caso, un passo FW è più sensato se d_fw non è anch'esso zero.\n",
    "                d = d_fw\n",
    "                gamma_max_ls = 1.0\n",
    "                direction_type = \"FW (fallback from Away)\"\n",
    "            else:\n",
    "                gamma_max_ls = alpha_v_t / (1.0 - alpha_v_t)\n",
    "                direction_type = \"Away\"\n",
    "        \n",
    "        gamma = line_search_generic(objective_function, x, d, A, gamma_max=gamma_max_ls, reg_type_info=reg_type)\n",
    "\n",
    "        # Aggiornamento di x (alpha_i)\n",
    "        if direction_type == \"FW\" or direction_type == \"FW (fallback from Away)\":\n",
    "            # x_new = (1-gamma)*x + gamma*s_vertex_fw\n",
    "            x = (1.0 - gamma) * x\n",
    "            x[s_index] += gamma\n",
    "        elif direction_type == \"Away\":\n",
    "            # x_new = (1+gamma)*x - gamma*v_vertex\n",
    "            x = (1.0 + gamma) * x\n",
    "            x[v_index] -= gamma\n",
    "        \n",
    "        x[x < 1e-10] = 0.0\n",
    "        current_sum = np.sum(x)\n",
    "        if abs(current_sum - 1.0) > 1e-7 and current_sum > 1e-9:\n",
    "            x /= current_sum\n",
    "            \n",
    "    return x\n",
    "\n",
    "def projected_gradient(A, reg_type='l2', max_iter=1000, tol=1e-6, lr_initial=0.1):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n\n",
    "    \n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "    \n",
    "    # Semplice backtracking line search per PG (adattato per massimizzazione)\n",
    "    lr = lr_initial\n",
    "    beta_bt = 0.5  # Fattore di riduzione per lr\n",
    "    c_bt = 1e-4    # Costante Armijo\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A) # Gradiente per massimizzazione\n",
    "        \n",
    "        # Backtracking line search\n",
    "        lr_current_iter = lr\n",
    "        f_x_current = objective_function(x, A) if reg_type=='l2' else objective_function(x,A,alpha=0.07,beta=5) # Valore corrente per massimizzazione\n",
    "\n",
    "        while True:\n",
    "            x_candidate_step = x + lr_current_iter * grad_orig # Passo di salita del gradiente\n",
    "            x_next = projection_simplex(x_candidate_step)      # Proiezione\n",
    "            \n",
    "            f_x_next = objective_function(x_next, A) if reg_type=='l2' else objective_function(x_next,A,alpha=0.07,beta=5)\n",
    "\n",
    "            # Condizione di Armijo per massimizzazione: f(x_new) >= f(x_old) + c * <grad_f_old, x_new - x_old>\n",
    "            if f_x_next >= f_x_current + c_bt * np.dot(grad_orig, x_next - x):\n",
    "                break\n",
    "            lr_current_iter *= beta_bt\n",
    "            if lr_current_iter < 1e-9: # Evita step troppo piccoli / loop infiniti\n",
    "                # print(f\"PG iter {t}: Line search lr too small. Breaking LS.\")\n",
    "                break\n",
    "        \n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            # print(f\"PG iter {t}: Change in x {np.linalg.norm(x_next - x):.2e} < tol. Converged.\")\n",
    "            break\n",
    "        x = x_next\n",
    "        # Adatta lr per la prossima iterazione (euristica comune, opzionale)\n",
    "        # if lr_current_iter < lr / (beta_bt * 2) and lr_current_iter > 1e-8 : lr = lr_current_iter * 2 \n",
    "        # else: lr = lr_initial * 0.99 # Decadimento lento\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "# ... (il resto del codice per extract_clique, load_graph, e il blocco if __name__ == \"__main__\"\n",
    "#      può rimanere come nella tua versione precedente, assicurandoti che chiami\n",
    "#      le funzioni corrette e gestisca i loro output).\n",
    "\n",
    "\n",
    "\n",
    "def extract_clique(x, A, threshold=1e-5):\n",
    "    \"\"\"Extract clique from solution vector.\"\"\"\n",
    "    S = np.where(x > threshold)[0]\n",
    "    if len(S) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by weight descending\n",
    "    sorted_indices = S[np.argsort(-x[S])]\n",
    "    clique = []\n",
    "    for i in sorted_indices:\n",
    "        # Check if vertex i is connected to all vertices in current clique\n",
    "        is_connected = True\n",
    "        for j in clique:\n",
    "            if A[i, j] == 0:\n",
    "                is_connected = False\n",
    "                break\n",
    "        if is_connected:\n",
    "            clique.append(i)\n",
    "    return clique\n",
    "\n",
    "# Load graph from MTX file\n",
    "def load_graph(file_path):\n",
    "    \"\"\"Load graph from MTX file and return adjacency matrix.\"\"\"\n",
    "    sparse_matrix = mmread(file_path)\n",
    "    A_dense = sparse_matrix.toarray()\n",
    "    np.fill_diagonal(A_dense, 0)  # Remove self-loops\n",
    "    return A_dense\n",
    "\n",
    "# Main experiment\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    graph_file = \"C:/Users/ricky/OneDrive/Desktop/OPT_PROJECT/data/brock200-4.mtx\"\n",
    "    reg_types = ['l2', 'l0']\n",
    "    algorithms = {\n",
    "        'FW': frank_wolfe,\n",
    "        'PFW': pairwise_frank_wolfe,\n",
    "        'AFW': away_step_frank_wolfe,\n",
    "        'PGD': projected_gradient\n",
    "    }\n",
    "    \n",
    "    # Load graph\n",
    "    A = load_graph(graph_file)\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    for reg_type in reg_types:\n",
    "        for algo_name, algo_func in algorithms.items():\n",
    "            try:\n",
    "                start_time = time.perf_counter()  # More precise timing\n",
    "                x = algo_func(A, reg_type=reg_type)\n",
    "                runtime = time.perf_counter() - start_time\n",
    "                clique = extract_clique(x, A)\n",
    "                clique_size = len(clique)\n",
    "                \n",
    "                # Verify clique validity\n",
    "                is_valid = all(A[i, j] == 1 for i in clique for j in clique if i != j)\n",
    "                \n",
    "                results.append({\n",
    "                    'Algorithm': algo_name,\n",
    "                    'Regularization': reg_type,\n",
    "                    'Clique Size': clique_size,\n",
    "                    'Valid Clique': is_valid,\n",
    "                    'Runtime (s)': runtime\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} with {reg_type}: {str(e)}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults for\", graph_file)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Graph size: {n}x{n}\")\n",
    "    print(\"=\"*60)\n",
    "    for res in results:\n",
    "        print(f\"{res['Algorithm']} + {res['Regularization']}:\")\n",
    "        print(f\"  Clique Size = {res['Clique Size']}\")\n",
    "        print(f\"  Valid Clique = {res['Valid Clique']}\")\n",
    "        print(f\"  Runtime = {res['Runtime (s)']:.6f} s\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
