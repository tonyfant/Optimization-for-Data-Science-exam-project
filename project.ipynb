{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a996b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "# from scipy.sparse import csr_matrix # Non usata se A Ã¨ densa\n",
    "import time\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_simplex(v, z=1.0):\n",
    "    \"\"\"Project vector v onto the simplex (sum(x)=z, x>=0).\"\"\"\n",
    "    n = len(v)\n",
    "    if n == 0: return np.array([])\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    \n",
    "    \n",
    "    rho_idx = -1\n",
    "    for i in range(n - 1, -1, -1): \n",
    "        if u[i] + (z - cssv[i]) / (i + 1) > 0: \n",
    "            rho_idx = i\n",
    "            break\n",
    "    \n",
    "\n",
    "    if rho_idx == -1: \n",
    "        if z == 0: return np.zeros(n)\n",
    "        \n",
    "        pass \n",
    "    theta = (cssv[rho_idx] - z) / (rho_idx + 1.0)\n",
    "    \n",
    "    proj_v = np.maximum(v - theta, 0)\n",
    "    \n",
    "   \n",
    "    current_sum = np.sum(proj_v)\n",
    "    if z > 1e-9: \n",
    "        if abs(current_sum - z) > 1e-7 * z: \n",
    "            if current_sum > 1e-9: \n",
    "                proj_v = proj_v * (z / current_sum)\n",
    "            else: \n",
    "                  pass\n",
    "    elif abs(current_sum) > 1e-7 : \n",
    "         proj_v = np.zeros(n)\n",
    "         \n",
    "    return proj_v\n",
    "\n",
    "\n",
    "def lmo(grad):\n",
    "    \"\"\"Linear Minimization Oracle for the simplex.\"\"\"\n",
    "    i = np.argmin(grad)\n",
    "    s = np.zeros_like(grad)\n",
    "    s[i] = 1.0\n",
    "    return s, i \n",
    "\n",
    "def f_l2(x, A):\n",
    "    \"\"\"L2-regularized objective function\"\"\"\n",
    "    return x @ A @ x + 0.5 * np.dot(x, x)\n",
    "\n",
    "def grad_l2(x, A):\n",
    "    \"\"\"Gradient for L2-regularized objective\"\"\"\n",
    "    \n",
    "    return 2 * (A @ x) + x\n",
    "\n",
    "def f_l0(x, A, alpha=0.07, beta=5):\n",
    "    \"\"\"L0-regularized objective function\"\"\"\n",
    "   \n",
    "    reg_term = alpha * np.sum(np.exp(-beta * np.clip(x, 0, None)) - 1.0)\n",
    "    return x @ A @ x + reg_term\n",
    "\n",
    "def grad_l0(x, A, alpha=0.07, beta=5):\n",
    "    \"\"\"Gradient for L0-regularized objective\"\"\"\n",
    "    return 2 * (A @ x) - alpha * beta * np.exp(-beta * np.clip(x, 0, None))\n",
    "\n",
    "\n",
    "def line_search_generic(objective_func, x_curr, d_curr, A_matrix, gamma_max=1.0, reg_type_info='l2', alpha_l0=0.07, beta_l0=5):\n",
    "    \"\"\"Line search to find perfect stepsize.\"\"\"\n",
    "    if np.linalg.norm(d_curr) < 1e-12: \n",
    "        return 0.0\n",
    "    \n",
    "    def func_to_minimize(gamma_ls):\n",
    "        x_candidate = x_curr + gamma_ls * d_curr \n",
    "        \n",
    "        # Project candidate for L0 to ensure non-negativity for the exp term\n",
    "        if reg_type_info == 'l0':\n",
    "            x_candidate = np.maximum(x_candidate, 0)\n",
    "\n",
    "        if reg_type_info == 'l2' :\n",
    "            val = objective_func(x_candidate, A_matrix) \n",
    "        else: # CORRECTED BLOCK\n",
    "            val = objective_func(x_candidate, A_matrix, alpha=alpha_l0, beta=beta_l0)\n",
    "        return -val # We minimize the negative objective to maximize the objective\n",
    "    \n",
    "    if not (np.isfinite(gamma_max) and gamma_max >= 0): gamma_max = 1.0\n",
    "    \n",
    "    if gamma_max < 1e-12: return 0.0\n",
    "\n",
    "    try:\n",
    "        res = minimize_scalar(func_to_minimize, bounds=(0, gamma_max), method='bounded')\n",
    "        gamma = res.x\n",
    "        if not np.isfinite(gamma): gamma = 0.0\n",
    "    except Exception:\n",
    "        gamma = 0.0 \n",
    "        \n",
    "    return np.clip(gamma, 0.0, gamma_max)\n",
    "\n",
    "def extract_clique(x, A, threshold=1e-5):\n",
    "    \"\"\"Extract clique from solution vector.\"\"\"\n",
    "    S = np.where(x > threshold)[0]\n",
    "    if len(S) == 0:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    sorted_indices = S[np.argsort(-x[S])]\n",
    "    clique = []\n",
    "    for i in sorted_indices:\n",
    "       \n",
    "        is_connected = True\n",
    "        for j in clique:\n",
    "            if A[i, j] == 0:\n",
    "                is_connected = False\n",
    "                break\n",
    "        if is_connected:\n",
    "            clique.append(i)\n",
    "    return clique\n",
    "\n",
    "\n",
    "def load_graph(file_path):\n",
    "    \"\"\"Load graph from MTX file and return adjacency matrix.\"\"\"\n",
    "    sparse_matrix = mmread(file_path)\n",
    "    A_dense = sparse_matrix.toarray()\n",
    "    np.fill_diagonal(A_dense, 0) \n",
    "    return A_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39ba63",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1f476",
   "metadata": {},
   "source": [
    "# Fank-Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frank_wolfe(A, reg_type='l2', max_iter=400, tol=1e-6, fixed_stepsize=None):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n \n",
    "\n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "\n",
    "    history = [objective_function(x, A)]\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A)  \n",
    "        grad_min = -grad_orig                   \n",
    "\n",
    "        s_vertex, _ = lmo(grad_min)\n",
    "        d = s_vertex - x\n",
    "\n",
    "        gap = grad_orig @ d\n",
    "\n",
    "        if gap < tol:\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "        if fixed_stepsize is not None:\n",
    "            gamma = fixed_stepsize\n",
    "        else:\n",
    "            if reg_type == 'l2':\n",
    "                gamma = line_search_generic(objective_function, x, d, A, gamma_max=1.0, reg_type_info='l2')\n",
    "            else: \n",
    "                gamma = line_search_generic(objective_function, x, d, A, gamma_max=1.0, reg_type_info='l0')\n",
    "       \n",
    "        x = x + gamma * d\n",
    "\n",
    "        history.append(objective_function(x, A))\n",
    "\n",
    "\n",
    "    return x, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda126ca",
   "metadata": {},
   "source": [
    "# Pairwise Frank-Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_frank_wolfe(A, reg_type='l2', max_iter=400, tol=1e-6, fixed_stepsize=None):\n",
    "    n = A.shape[0]\n",
    "    x0_idx = np.random.randint(n)\n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0  \n",
    "\n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "\n",
    "    history = [objective_function(x, A)]\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A)\n",
    "        grad_min = -grad_orig\n",
    "\n",
    "        s_vertex_fw, s_index = lmo(grad_min)  \n",
    "\n",
    "        d_for_gap = s_vertex_fw - x\n",
    "        gap = grad_orig @ d_for_gap\n",
    "        if gap < tol:\n",
    "            break\n",
    "\n",
    "        current_support_indices = np.where(x > 1e-9)[0]\n",
    "        if len(current_support_indices) == 0:\n",
    "            x.fill(0.0)\n",
    "            x[s_index] = 1.0\n",
    "            \n",
    "            history.append(objective_function(x, A))\n",
    "            continue\n",
    "\n",
    "        v_index = current_support_indices[np.argmax(grad_orig[current_support_indices])]\n",
    "\n",
    "        if s_index == v_index:\n",
    "            d = s_vertex_fw - x \n",
    "            gamma_max_ls = 1.0\n",
    "        else:\n",
    "            v_vertex = np.zeros(n)\n",
    "            v_vertex[v_index] = 1.0\n",
    "            d = s_vertex_fw - v_vertex \n",
    "            gamma_max_ls = x[v_index]\n",
    "\n",
    "        if fixed_stepsize is not None:\n",
    "            gamma = min(fixed_stepsize, gamma_max_ls)\n",
    "        else:\n",
    "            gamma = line_search_generic(objective_function, x, d, A, gamma_max=gamma_max_ls, reg_type_info=reg_type)\n",
    "\n",
    "        if s_index == v_index:\n",
    "            x = (1.0 - gamma) * x + gamma * s_vertex_fw\n",
    "        else:\n",
    "            x[s_index] += gamma\n",
    "            x[v_index] -= gamma\n",
    "\n",
    "        x[x < 1e-10] = 0.0\n",
    "        current_sum = np.sum(x)\n",
    "        if abs(current_sum - 1.0) > 1e-7 and current_sum > 1e-9:\n",
    "            x /= current_sum\n",
    "\n",
    "\n",
    "        history.append(objective_function(x, A))\n",
    "\n",
    "\n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c07c79",
   "metadata": {},
   "source": [
    "# Away-step Frank-Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def away_step_frank_wolfe(A, reg_type='l2', max_iter=400, tol=1e-6, fixed_stepsize=None):\n",
    "    n = A.shape[0]\n",
    "\n",
    "    x0_idx = np.random.randint(n)\n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0\n",
    "\n",
    "    objective_function = f_l2 if reg_type == 'l2' else f_l0\n",
    "    gradient_function = grad_l2 if reg_type == 'l2' else grad_l0\n",
    "\n",
    "    history = [objective_function(x, A)]\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad_orig = gradient_function(x, A)\n",
    "        grad_min = -grad_orig\n",
    "\n",
    "        s_vertex_fw, s_index = lmo(grad_min)\n",
    "        d_fw = s_vertex_fw - x\n",
    "        \n",
    "        gap_fw = grad_orig @ d_fw\n",
    "        if gap_fw < tol:\n",
    "            break\n",
    "\n",
    "        current_support_indices = np.where(x > 1e-9)[0]\n",
    "        if len(current_support_indices) == 0:\n",
    "            x.fill(0.0)\n",
    "            x[s_index] = 1.0\n",
    "            history.append(objective_function(x, A))\n",
    "            continue\n",
    "            \n",
    "        v_index = current_support_indices[np.argmax(grad_orig[current_support_indices])]\n",
    "        v_vertex = np.zeros(n)\n",
    "        v_vertex[v_index] = 1.0\n",
    "        d_away = x - v_vertex\n",
    "        \n",
    "        potential_progress_fw = grad_orig @ d_fw\n",
    "        potential_progress_away = grad_orig @ d_away\n",
    "        \n",
    "        direction_type = \"\"\n",
    "        if potential_progress_fw >= potential_progress_away:\n",
    "            d = d_fw\n",
    "            gamma_max_ls = 1.0\n",
    "            direction_type = \"FW\"\n",
    "        else:\n",
    "            d = d_away\n",
    "            alpha_v_t = x[v_index]\n",
    "            if abs(1.0 - alpha_v_t) < 1e-9 or np.linalg.norm(d_away) < 1e-9:\n",
    "                d = d_fw\n",
    "                gamma_max_ls = 1.0\n",
    "                direction_type = \"FW (fallback from Away)\"\n",
    "            else:\n",
    "                gamma_max_ls = alpha_v_t / (1.0 - alpha_v_t)\n",
    "                direction_type = \"Away\"\n",
    "        \n",
    "        if fixed_stepsize is not None:\n",
    "            gamma = min(fixed_stepsize, gamma_max_ls)\n",
    "        else:\n",
    "            gamma = line_search_generic(objective_function, x, d, A, gamma_max=gamma_max_ls, reg_type_info=reg_type)\n",
    "\n",
    "        if direction_type.startswith(\"FW\"):\n",
    "            x = (1.0 - gamma) * x\n",
    "            x[s_index] += gamma\n",
    "        elif direction_type == \"Away\":\n",
    "            x = (1.0 + gamma) * x\n",
    "            x[v_index] -= gamma\n",
    "        \n",
    "        x[x < 1e-10] = 0.0\n",
    "        current_sum = np.sum(x)\n",
    "        if abs(current_sum - 1.0) > 1e-7 and current_sum > 1e-9:\n",
    "            x /= current_sum\n",
    "        \n",
    "\n",
    "        history.append(objective_function(x, A))\n",
    "            \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377356a",
   "metadata": {},
   "source": [
    "# Projected Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient(A, reg_type='l2', max_iter=200, tol=1e-6, lr_initial=0.1, fixed_learning_rate=None):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n  \n",
    "\n",
    "    if reg_type == 'l2':\n",
    "        objective_function = f_l2\n",
    "        gradient_function = grad_l2\n",
    "    else:  \n",
    "        objective_function = f_l0\n",
    "        gradient_function = grad_l0\n",
    "\n",
    "    history = [objective_function(x, A)]\n",
    "\n",
    "    for t in range(max_iter):\n",
    "       \n",
    "        grad_orig = gradient_function(x, A)\n",
    "\n",
    "        \n",
    "        if fixed_learning_rate is not None:\n",
    "            gamma = fixed_learning_rate\n",
    "        else:\n",
    "            gamma = line_search_generic(\n",
    "                objective_func=objective_function,\n",
    "                x_curr=x,\n",
    "                d_curr=grad_orig, \n",
    "                A_matrix=A,\n",
    "                gamma_max=lr_initial,\n",
    "                reg_type_info=reg_type\n",
    "            )\n",
    "\n",
    "        x_candidate_unprojected = x + gamma * grad_orig\n",
    "\n",
    "        x_next = projection_simplex(x_candidate_unprojected)\n",
    "\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        x = x_next\n",
    "\n",
    "\n",
    "        history.append(objective_function(x, A))\n",
    "\n",
    "    return x, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ab897",
   "metadata": {},
   "source": [
    "# Experiments execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e95e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_files = [\"./data/C250-9.mtx\", \"./data/C500-9.mtx\", \"./data/C2000-9.mtx\"]\n",
    "reg_types = ['l2', 'l0']\n",
    "algorithms = {\n",
    "    'FW': frank_wolfe,\n",
    "    'PFW': pairwise_frank_wolfe,\n",
    "    'AFW': away_step_frank_wolfe,\n",
    "    'PGD': projected_gradient\n",
    "}\n",
    "stepsize_options = [None, 0.15]\n",
    "\n",
    "\n",
    "results_list = []\n",
    "for graph_file in graph_files:\n",
    "    print(f\"\\n--- Processing graph file: {graph_file} ---\")\n",
    "    try:\n",
    "        A = load_graph(graph_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Errore: File del grafo '{graph_file}' non trovato. Salto questo file.\")\n",
    "        continue\n",
    "    n = A.shape[0]\n",
    "\n",
    "    \n",
    "    for step in stepsize_options:\n",
    "        step_label = 'Linear Search' if step is None else f'Fixed ({step})'\n",
    "        print(f\"--- Running experiments with stepsize: {step_label} for {graph_file} ---\")\n",
    "        for reg_type in reg_types:\n",
    "            for algo_name, algo_func in algorithms.items():\n",
    "                print(f\"Running: {algo_name} - Reg: {reg_type} - Step: {step_label}\")\n",
    "                try:\n",
    "                    start_time = time.perf_counter()\n",
    "\n",
    "                    kwargs = {'A': A, 'reg_type': reg_type, 'max_iter': 100}\n",
    "                    if algo_name == 'PGD':\n",
    "                        kwargs['fixed_learning_rate'] = step \n",
    "                        if step is None:\n",
    "                            kwargs['lr_initial'] = 0.1\n",
    "                    else:\n",
    "                        kwargs['fixed_stepsize'] = step\n",
    "\n",
    "                    x, history = algo_func(**kwargs)\n",
    "\n",
    "                    runtime = time.perf_counter() - start_time\n",
    "                    clique = extract_clique(x, A)\n",
    "                    clique_size = len(clique)\n",
    "\n",
    "                    is_valid = True\n",
    "                    if clique_size > 1:\n",
    "                        is_valid = all(A[i, j] == 1 for i in clique for j in clique if i != j)\n",
    "\n",
    "                    results_list.append({\n",
    "                        'Graph File': graph_file,\n",
    "                        'Algorithm': algo_name,\n",
    "                        'Regularization': reg_type,\n",
    "                        'Stepsize': step_label,\n",
    "                        'Clique Size': clique_size,\n",
    "                        'Valid Clique': is_valid,\n",
    "                        'Runtime (s)': runtime,\n",
    "                        'History': history\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {algo_name} with {reg_type} and step {step_label} for {graph_file}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    results_list.append({\n",
    "                        'Graph File': graph_file,\n",
    "                        'Algorithm': algo_name, 'Regularization': reg_type,\n",
    "                        'Stepsize': step_label, 'Clique Size': -1,\n",
    "                        'Valid Clique': False, 'Runtime (s)': -1,\n",
    "                        'Error': str(e), 'History': []\n",
    "                    })\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                 FINAL RESULTS TABLE (Pandas DataFrame)                  \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if results_list:\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "    columns_to_show = [col for col in df_results.columns if col != 'History']\n",
    "\n",
    "\n",
    "    display_order = ['Graph File', 'Algorithm', 'Regularization', 'Stepsize', 'Clique Size', 'Valid Clique', 'Runtime (s)']\n",
    "    if 'Error' in columns_to_show:\n",
    "        display_order.append('Error')\n",
    "\n",
    "    final_columns = [col for col in display_order if col in columns_to_show]\n",
    "\n",
    "    final_columns.extend([col for col in columns_to_show if col not in final_columns])\n",
    "\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df_results[final_columns].to_string())\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28d1ff",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a935b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# Definisce colori e stili per una migliore leggibilitÃ \n",
    "palette = {\n",
    "    'FW': ('#1f77b4', '-'),   # Blu\n",
    "    'PFW': ('#ff7f0e', '--'), # Arancione\n",
    "    'AFW': ('#2ca02c', ':'),   # Verde\n",
    "    'PGD': ('#d62728', '-.')  # Rosso\n",
    "}\n",
    "\n",
    "# Ottiene la lista unica di file di grafo dal DataFrame\n",
    "unique_graphs = df_results['Graph File'].unique()\n",
    "\n",
    "# Itera su ogni file di grafo per creare una figura separata\n",
    "for graph in unique_graphs:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharex=True)\n",
    "    fig.suptitle(f\"Analisi della Convergenza per il Grafo: {graph.split('/')[-1]}\", fontsize=20, fontweight='bold')\n",
    "\n",
    "    # Filtra il DataFrame per il grafo corrente\n",
    "    df_graph = df_results[df_results['Graph File'] == graph]\n",
    "\n",
    "    # Mappa per titoli e filtri\n",
    "    plot_map = {\n",
    "        (0, 0): ('Regolarizzazione L2 - Stepsize Fissa', (df_graph['Regularization'] == 'l2') & (df_graph['Stepsize'].str.contains('Fixed'))),\n",
    "        (0, 1): ('Regolarizzazione L2 - Stepsize Dinamica (Linear Search)', (df_graph['Regularization'] == 'l2') & (df_graph['Stepsize'] == 'Linear Search')),\n",
    "        (1, 0): ('Regolarizzazione L0 - Stepsize Fissa', (df_graph['Regularization'] == 'l0') & (df_graph['Stepsize'].str.contains('Fixed'))),\n",
    "        (1, 1): ('Regolarizzazione L0 - Stepsize Dinamica (Linear Search)', (df_graph['Regularization'] == 'l0') & (df_graph['Stepsize'] == 'Linear Search'))\n",
    "    }\n",
    "\n",
    "    # Itera sulla mappa per creare ogni subplot\n",
    "    for (row, col), (title, condition) in plot_map.items():\n",
    "        ax = axes[row, col]\n",
    "        df_subset = df_graph[condition]\n",
    "\n",
    "        if df_subset.empty:\n",
    "            ax.text(0.5, 0.5, 'Nessun dato disponibile', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            # Plotta la history per ogni algoritmo\n",
    "            for _, result_row in df_subset.iterrows():\n",
    "                algo = result_row['Algorithm']\n",
    "                history = result_row['History']\n",
    "                if history: # Controlla se la history non Ã¨ vuota\n",
    "                    color, style = palette.get(algo, ('black', '-'))\n",
    "                    ax.plot(history, label=algo, color=color, linestyle=style, linewidth=2)\n",
    "\n",
    "        # Impostazioni del subplot\n",
    "        ax.set_title(title, fontsize=14, fontweight='medium')\n",
    "        ax.set_ylabel(\"Valore Funzione Obiettivo\", fontsize=12)\n",
    "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax.legend()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "\n",
    "    # Impostazioni comuni per le righe di subplot\n",
    "    for i in range(2):\n",
    "        axes[i, 0].set_ylabel(\"Valore Funzione Obiettivo\", fontsize=12)\n",
    "    for i in range(2):\n",
    "        axes[1, i].set_xlabel(\"Iterazioni\", fontsize=12)\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    # Salva la figura invece di mostrarla, per un migliore output\n",
    "    # plt.savefig(f\"plot_{graph.split('/')[-1].replace('.mtx', '')}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_valid_results = df_results[(df_results['Valid Clique'] == True) & (df_results['Clique Size'] > 0)]\n",
    "\n",
    "if 'Error' in df_valid_results.columns:\n",
    "    df_valid_results = df_valid_results[df_valid_results['Error'].isnull()]\n",
    "\n",
    "if df_valid_results.empty:\n",
    "    print(\"Warning: No valid results found after filtering. Plots might be empty or not generated.\")\n",
    "else:\n",
    "    print(f\"Number of valid results for plotting: {len(df_valid_results)}\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=df_valid_results, x='Algorithm', y='Clique Size', hue='Stepsize', palette='viridis', errorbar=None)\n",
    "    plt.title('Maximum Clique Size by Algorithm and Stepsize', fontsize=16)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel('Maximum Clique Size', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Stepsize')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=df_valid_results, x='Algorithm', y='Runtime (s)', hue='Stepsize', palette='mako', errorbar=None)\n",
    "    plt.title('Average Runtime by Algorithm and Stepsize', fontsize=16)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel('Average Runtime (s)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Stepsize')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    grouped_clique = df_valid_results.groupby(['Algorithm', 'Regularization', 'Stepsize'])['Clique Size'].mean().reset_index()\n",
    "    \n",
    "    if not grouped_clique.empty:\n",
    "        g = sns.catplot(data=grouped_clique, x='Algorithm', y='Clique Size', hue='Stepsize',\n",
    "                        col='Regularization', kind='bar', palette='pastel', height=5, aspect=1.2)\n",
    "        g.fig.suptitle('Impact of Stepsize and Regularization on Clique Size', y=1.03)\n",
    "        g.set_axis_labels(\"Algorithm\", \"Average Clique Size\")\n",
    "        g.set_titles(\"Regularization: {col_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough data for the 'Impact of Stepsize on Clique Size' plot.\")\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(data=df_valid_results, x='Algorithm', y='Clique Size', hue='Regularization', palette='Set2')\n",
    "    plt.title('Impact of Regularization on Clique Size Distribution', fontsize=16)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel('Clique Size', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Regularization')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612946b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assicurati che le librerie necessarie siano importate.\n",
    "# Il tuo DataFrame 'df_results' dovrebbe essere disponibile a questo punto.\n",
    "\n",
    "# --- Inizio del codice per la visualizzazione ---\n",
    "\n",
    "# 1. Filtriamo i risultati per includere solo le clique valide, per evitare di plottare dati errati.\n",
    "#    Usiamo .copy() per evitare warning da parte di pandas.\n",
    "df_valid_results = df_results[df_results['Valid Clique']].copy()\n",
    "\n",
    "# Se non ci sono risultati validi, stampa un messaggio ed esci.\n",
    "if df_valid_results.empty:\n",
    "    print(\"Nessun risultato valido da visualizzare.\")\n",
    "else:\n",
    "    # 2. Otteniamo la lista unica degli algoritmi testati\n",
    "    algorithms = df_valid_results['Algorithm'].unique()\n",
    "    \n",
    "    # 3. Creiamo una griglia di subplot. 2 righe, 2 colonne.\n",
    "    #    Aumentiamo la figsize per dare spazio a tutti i grafici.\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    \n",
    "    # \"Appiattiamo\" l'array di assi per iterare piÃ¹ facilmente con un singolo ciclo for\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # 4. Iteriamo su ogni algoritmo e sul corrispondente asse (subplot)\n",
    "    for i, algo_name in enumerate(algorithms):\n",
    "        ax = axes[i] # Selezioniamo l'asse corrente\n",
    "        \n",
    "        # Filtriamo il DataFrame per ottenere solo i dati dell'algoritmo corrente\n",
    "        df_algo = df_valid_results[df_valid_results['Algorithm'] == algo_name]\n",
    "        \n",
    "        # Creiamo lo scatter plot sull'asse corrente\n",
    "        sns.scatterplot(\n",
    "            data=df_algo,\n",
    "            x='Runtime (s)',\n",
    "            y='Clique Size',\n",
    "            hue='Regularization', # Il colore dipende dalla regolarizzazione (come richiesto)\n",
    "            style='Stepsize',     # Usiamo lo stile del marker per differenziare lo stepsize\n",
    "            size='Stepsize',      # Usiamo anche la dimensione per lo stepsize\n",
    "            sizes=(100, 250),     # Intervallo di dimensioni per i punti\n",
    "            palette='viridis',    # Un'altra palette di colori, per esempio 'viridis' o 'plasma'\n",
    "            ax=ax                 # Specifichiamo su quale subplot disegnare\n",
    "        )\n",
    "        \n",
    "        # Impostiamo il titolo e le etichette per ogni subplot\n",
    "        ax.set_title(f\"Performance per l'algoritmo: {algo_name}\", fontsize=14, weight='bold')\n",
    "        ax.set_xlabel('Runtime (s)', fontsize=12)\n",
    "        ax.set_ylabel('Clique Size', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.legend(title='Legenda') # Mostra la legenda su ogni subplot\n",
    "\n",
    "    # 5. Se il numero di algoritmi non Ã¨ un multiplo di 2 (es. 3), nascondiamo gli assi vuoti.\n",
    "    for i in range(len(algorithms), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # 6. Aggiungiamo un titolo principale per l'intera figura\n",
    "    fig.suptitle('Confronto Performance: Dimensione Clique vs. Runtime per Algoritmo', fontsize=20, y=0.98)\n",
    "\n",
    "    # 7. Ottimizziamo il layout per evitare sovrapposizioni di titoli ed etichette\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # rect lascia spazio per il suptitle\n",
    "\n",
    "    # 8. Mostriamo il grafico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Elaborazione del file: ./data/C250-9.mtx ---\n",
      "Grafo caricato con 250 nodi.\n",
      "--- Esecuzione con strategia di passo: Optimal Gamma (LS) ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "--- Esecuzione con strategia di passo: Armijo Rule ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "\n",
      "--- Elaborazione del file: ./data/C500-9.mtx ---\n",
      "Grafo caricato con 500 nodi.\n",
      "--- Esecuzione con strategia di passo: Optimal Gamma (LS) ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "--- Esecuzione con strategia di passo: Armijo Rule ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "\n",
      "--- Elaborazione del file: ./data/C2000-9.mtx ---\n",
      "Grafo caricato con 2000 nodi.\n",
      "--- Esecuzione con strategia di passo: Optimal Gamma (LS) ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "--- Esecuzione con strategia di passo: Armijo Rule ---\n",
      "Esecuzione: FW - Reg: l2\n",
      "Esecuzione: PFW - Reg: l2\n",
      "Esecuzione: AFW - Reg: l2\n",
      "Esecuzione: PGD - Reg: l2\n",
      "Esecuzione: FW - Reg: l0\n",
      "Esecuzione: PFW - Reg: l0\n",
      "Esecuzione: AFW - Reg: l0\n",
      "Esecuzione: PGD - Reg: l0\n",
      "\n",
      "=====================================================================================\n",
      "                              RIEPILOGO FINALE DEI RISULTATI                              \n",
      "=====================================================================================\n",
      "     Graph File Algorithm Reg            Stepsize  Clique Size  Valid  Runtime (s)\n",
      "0    C250-9.mtx        FW  l2  Optimal Gamma (LS)           35   True     0.063030\n",
      "1    C250-9.mtx       PFW  l2  Optimal Gamma (LS)           34   True     0.062013\n",
      "2    C250-9.mtx       AFW  l2  Optimal Gamma (LS)           32   True     0.016002\n",
      "3    C250-9.mtx       PGD  l2  Optimal Gamma (LS)           34   True     0.082427\n",
      "4    C250-9.mtx        FW  l0  Optimal Gamma (LS)           35   True     0.106035\n",
      "5    C250-9.mtx       PFW  l0  Optimal Gamma (LS)           35   True     0.068464\n",
      "6    C250-9.mtx       AFW  l0  Optimal Gamma (LS)           33   True     0.029384\n",
      "7    C250-9.mtx       PGD  l0  Optimal Gamma (LS)           34   True     0.097429\n",
      "8    C250-9.mtx        FW  l2         Armijo Rule           35   True     0.035382\n",
      "9    C250-9.mtx       PFW  l2         Armijo Rule           31   True     0.037994\n",
      "10   C250-9.mtx       AFW  l2         Armijo Rule           35   True     0.040025\n",
      "11   C250-9.mtx       PGD  l2         Armijo Rule           34   True     0.025468\n",
      "12   C250-9.mtx        FW  l0         Armijo Rule           35   True     0.041807\n",
      "13   C250-9.mtx       PFW  l0         Armijo Rule           32   True     0.036265\n",
      "14   C250-9.mtx       AFW  l0         Armijo Rule           32   True     0.045018\n",
      "15   C250-9.mtx       PGD  l0         Armijo Rule           34   True     0.030408\n",
      "16   C500-9.mtx        FW  l2  Optimal Gamma (LS)           42   True     0.760046\n",
      "17   C500-9.mtx       PFW  l2  Optimal Gamma (LS)           37   True     0.658244\n",
      "18   C500-9.mtx       AFW  l2  Optimal Gamma (LS)           40   True     0.256864\n",
      "19   C500-9.mtx       PGD  l2  Optimal Gamma (LS)           39   True     1.055961\n",
      "20   C500-9.mtx        FW  l0  Optimal Gamma (LS)           43   True     1.221337\n",
      "21   C500-9.mtx       PFW  l0  Optimal Gamma (LS)           39   True     0.656432\n",
      "22   C500-9.mtx       AFW  l0  Optimal Gamma (LS)           39   True     0.389250\n",
      "23   C500-9.mtx       PGD  l0  Optimal Gamma (LS)           39   True     1.052229\n",
      "24   C500-9.mtx        FW  l2         Armijo Rule           43   True     0.567394\n",
      "25   C500-9.mtx       PFW  l2         Armijo Rule           40   True     0.535182\n",
      "26   C500-9.mtx       AFW  l2         Armijo Rule           39   True     0.591380\n",
      "27   C500-9.mtx       PGD  l2         Armijo Rule           39   True     0.329965\n",
      "28   C500-9.mtx        FW  l0         Armijo Rule           43   True     0.575026\n",
      "29   C500-9.mtx       PFW  l0         Armijo Rule           40   True     0.474186\n",
      "30   C500-9.mtx       AFW  l0         Armijo Rule           37   True     0.556355\n",
      "31   C500-9.mtx       PGD  l0         Armijo Rule           39   True     0.332899\n",
      "32  C2000-9.mtx        FW  l2  Optimal Gamma (LS)           56   True    11.376604\n",
      "33  C2000-9.mtx       PFW  l2  Optimal Gamma (LS)           52   True     9.684422\n",
      "34  C2000-9.mtx       AFW  l2  Optimal Gamma (LS)           52   True     5.215463\n",
      "35  C2000-9.mtx       PGD  l2  Optimal Gamma (LS)           52   True    15.266586\n",
      "36  C2000-9.mtx        FW  l0  Optimal Gamma (LS)           56   True    17.627325\n",
      "37  C2000-9.mtx       PFW  l0  Optimal Gamma (LS)           56   True     9.856932\n",
      "38  C2000-9.mtx       AFW  l0  Optimal Gamma (LS)           56   True     8.756498\n",
      "39  C2000-9.mtx       PGD  l0  Optimal Gamma (LS)           52   True    15.321364\n",
      "40  C2000-9.mtx        FW  l2         Armijo Rule           48   True     8.813439\n",
      "41  C2000-9.mtx       PFW  l2         Armijo Rule           52   True     7.031647\n",
      "42  C2000-9.mtx       AFW  l2         Armijo Rule           52   True     8.834062\n",
      "43  C2000-9.mtx       PGD  l2         Armijo Rule           52   True     5.145623\n",
      "44  C2000-9.mtx        FW  l0         Armijo Rule           46   True     8.719711\n",
      "45  C2000-9.mtx       PFW  l0         Armijo Rule           56   True     6.920611\n",
      "46  C2000-9.mtx       AFW  l0         Armijo Rule           52   True     9.871722\n",
      "47  C2000-9.mtx       PGD  l0         Armijo Rule           52   True     5.295537\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SCRIPT PER L'ANALISI DI ALGORITMI DI OTTIMIZZAZIONE\n",
    "# SUL PROBLEMA DELLA MASSIMA CLIQUE REGOLARIZZATA\n",
    "#\n",
    "# Include:\n",
    "# - Algoritmi: Frank-Wolfe, Pairwise FW, Away-Step FW, Projected Gradient\n",
    "# - Strategie di passo: Ricerca Lineare Esatta, Regola di Armijo\n",
    "# - Problemi: Regolarizzazione L2 e L0-approssimata\n",
    "# ======================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize_scalar\n",
    "import time\n",
    "from scipy.io import mmread\n",
    "\n",
    "# ======================================================================\n",
    "# --- 1. FUNZIONI AUSILIARIE ---\n",
    "# ======================================================================\n",
    "\n",
    "def lmo(grad):\n",
    "    i = np.argmin(grad)\n",
    "    s = np.zeros_like(grad)\n",
    "    s[i] = 1.0\n",
    "    return s, i\n",
    "\n",
    "def projection_simplex(v, z=1):\n",
    "    n_features = v.shape[0]\n",
    "    \n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u) - z\n",
    "    ind = np.arange(n_features) + 1\n",
    "    cond = u * ind > cssv\n",
    "    \n",
    "    if np.sum(cond) > 0:\n",
    "        rho = ind[cond][-1]\n",
    "        theta = cssv[cond][-1] / float(rho)\n",
    "        w = np.maximum(v - theta, 0)\n",
    "    else:\n",
    "        w = np.zeros_like(v)\n",
    "        if v.size > 0: w[np.argmax(v)] = z\n",
    "            \n",
    "    return w\n",
    "\n",
    "def extract_clique(x, A):\n",
    "    if x is None or not np.all(np.isfinite(x)):\n",
    "        return []\n",
    "    \n",
    "    support_indices = np.where(x > 1e-3)[0]\n",
    "    if len(support_indices) == 0: \n",
    "        return []\n",
    "    \n",
    "    # Ordina gli indici dei nodi per peso decrescente\n",
    "    sorted_support = support_indices[np.argsort(-x[support_indices])]\n",
    "    \n",
    "    clique = []\n",
    "    for i in sorted_support:\n",
    "        # Verifica se il nodo i Ã¨ connesso a tutti i nodi giÃ  nella cricca\n",
    "        is_connected_to_all = all(A[i, j] == 1 for j in clique)\n",
    "        if is_connected_to_all:\n",
    "            clique.append(i)\n",
    "    return clique\n",
    "\n",
    "def validate_clique(clique, A):\n",
    "    \"\"\"Verifica se un dato insieme di nodi forma una cricca.\"\"\"\n",
    "    if len(clique) <= 1: \n",
    "        return True\n",
    "\n",
    "    subgraph = A[np.ix_(clique, clique)]\n",
    "    # In una cricca, tutti gli elementi fuori diagonale devono essere 1.\n",
    "    # La somma deve essere il numero di archi in un grafo completo.\n",
    "    expected_edges = len(clique) * (len(clique) - 1)\n",
    "    return np.sum(subgraph) == expected_edges\n",
    "\n",
    "\n",
    "def load_graph(filepath):\n",
    "    \"\"\"Carica un grafo da file .mtx e restituisce la sua matrice di adiacenza.\"\"\"\n",
    "    sparse_matrix = mmread(filepath)\n",
    "\n",
    "    n = sparse_matrix.shape[0]\n",
    "    A = sparse_matrix.astype(int).toarray()\n",
    "    \n",
    "    # Assicura che sia una matrice di adiacenza 0/1, simmetrica, con diagonale nulla\n",
    "    A = np.ceil((A + A.T) / 2.0).astype(int)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    return A\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# --- 2. FUNZIONI OBIETTIVO E GRADIENTE (per Massimizzazione) ---\n",
    "# ======================================================================\n",
    "\n",
    "def f_l2(x, A, alpha=0.5): \n",
    "    return x.T @ A @ x + alpha * np.dot(x, x)\n",
    "\n",
    "def grad_l2(x, A, alpha=0.5): \n",
    "    return 2 * A @ x + alpha * 2 * x\n",
    "\n",
    "def f_l0(x, A, alpha=0.07, beta=5): \n",
    "    return x.T @ A @ x + alpha * np.sum(np.exp(-beta * x) - 1)\n",
    "\n",
    "def grad_l0(x, A, alpha=0.07, beta=5): \n",
    "    return 2 * A @ x - alpha * beta * np.exp(-beta * x)\n",
    "\n",
    "# ======================================================================\n",
    "# --- 3. STRATEGIE DI STEP SIZE ---\n",
    "# ======================================================================\n",
    "\n",
    "def exact_line_search(objective_func, x_curr, d_curr, A_matrix, grad_f_x, gamma_max=1.0, **obj_kwargs):\n",
    "    \"\"\"Ricerca lineare esatta (\"Optimal Gamma\").\"\"\"\n",
    "    if np.linalg.norm(d_curr) < 1e-12: \n",
    "        return 0.0\n",
    "    \n",
    "    def func_to_minimize(gamma_ls):\n",
    "        val = objective_func(x_curr + gamma_ls * d_curr, A_matrix, **obj_kwargs)\n",
    "        return -val # Minimizza il negativo per massimizzare l'obiettivo\n",
    "        \n",
    "    if not (np.isfinite(gamma_max) and gamma_max >= 0):\n",
    "        gamma_max = 1.0\n",
    "\n",
    "    if gamma_max < 1e-12:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        res = minimize_scalar(func_to_minimize, bounds=(0, gamma_max), method='bounded')\n",
    "        gamma = res.x\n",
    "        if not np.isfinite(gamma): \n",
    "            gamma = 0.0\n",
    "    except Exception:\n",
    "        gamma = 0.0\n",
    "        \n",
    "    return np.clip(gamma, 0.0, gamma_max)\n",
    "\n",
    "def armijo_line_search(objective_func, x_curr, d_curr, A_matrix, grad_f_x, gamma_max=1.0, **obj_kwargs):\n",
    "    \"\"\"Ricerca lineare inesatta con regola di Armijo e backtracking.\"\"\"\n",
    "    if np.linalg.norm(d_curr) < 1e-12: return 0.0\n",
    "    \n",
    "    gamma = gamma_max  # Inizia con il passo massimo possibile\n",
    "    c1 = 1e-4          # Parametro standard di Armijo\n",
    "    tau = 0.5          # Fattore di riduzione del backtracking\n",
    "    \n",
    "    f_x = objective_func(x_curr, A_matrix, **obj_kwargs)\n",
    "    progress_expected = np.dot(grad_f_x, d_curr)\n",
    "\n",
    "    # La direzione deve essere di salita per la massimizzazione\n",
    "    if progress_expected <= 0: return 0.0 \n",
    "\n",
    "    for _ in range(20): # Limita le iterazioni del backtracking\n",
    "        f_x_new = objective_func(x_curr + gamma * d_curr, A_matrix, **obj_kwargs)\n",
    "        # Condizione di Armijo per la massimizzazione\n",
    "        if f_x_new >= f_x + c1 * gamma * progress_expected:\n",
    "            return gamma\n",
    "        gamma *= tau\n",
    "        \n",
    "    return 0.0 # Il backtracking non ha trovato un passo valido\n",
    "\n",
    "# ======================================================================\n",
    "# --- 4. ALGORITMI DI OTTIMIZZAZIONE ---\n",
    "# ======================================================================\n",
    "\n",
    "def frank_wolfe(A, reg_type, max_iter, tol, step_func):\n",
    "    \"\"\"Implementazione del Frank-Wolfe Classico.\"\"\"\n",
    "    n = A.shape[0]\n",
    "    # FW parte spesso dal baricentro per una migliore esplorazione iniziale\n",
    "    x = np.ones(n) / n\n",
    "    \n",
    "    # Seleziona le funzioni e i parametri corretti\n",
    "    if reg_type == 'l2':\n",
    "        obj_func, grad_func, obj_kwargs = f_l2, grad_l2, {'alpha': 0.5}\n",
    "    else:\n",
    "        obj_func, grad_func, obj_kwargs = f_l0, grad_l0, {'alpha': 0.07, 'beta': 5}\n",
    "    \n",
    "    history = [obj_func(x, A, **obj_kwargs)]\n",
    "    x_best = np.copy(x)\n",
    "    f_best = history[0]\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        grad = grad_func(x, A, **obj_kwargs)\n",
    "        s_vertex, _ = lmo(-grad)\n",
    "        d = s_vertex - x\n",
    "        gap = np.dot(grad, d)\n",
    "        if gap < tol: break\n",
    "        \n",
    "        gamma = step_func(obj_func, x, d, A, grad, 1.0, **obj_kwargs)\n",
    "        x = (1 - gamma) * x + gamma * s_vertex\n",
    "\n",
    "        current_f = obj_func(x, A, **obj_kwargs)\n",
    "        history.append(current_f)\n",
    "        if current_f > f_best:\n",
    "            f_best = current_f\n",
    "            x_best = np.copy(x)\n",
    "            \n",
    "    return x_best, history\n",
    "\n",
    "def pairwise_frank_wolfe(A, reg_type, max_iter, tol, step_func):\n",
    "    \"\"\"Implementazione Corretta del Pairwise Frank-Wolfe.\"\"\"\n",
    "    n = A.shape[0]\n",
    "\n",
    "    x0_idx = np.random.randint(n)\n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0\n",
    "    \n",
    "    if reg_type == 'l2':\n",
    "        obj_func = f_l2\n",
    "        grad_func = grad_l2\n",
    "        obj_kwargs = {'alpha': 0.5}\n",
    "    else:\n",
    "        obj_func = f_l0\n",
    "        grad_func = grad_l0\n",
    "        obj_kwargs = {'alpha': 0.07, 'beta': 5}\n",
    "        \n",
    "    history = [obj_func(x, A, **obj_kwargs)]\n",
    "    x_best = np.copy(x)\n",
    "    f_best = history[0]\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        grad = grad_func(x, A, **obj_kwargs)\n",
    "        s_vertex, s_idx = lmo(-grad)\n",
    "        d_gap_vec = s_vertex - x\n",
    "        gap = np.dot(grad, d_gap_vec)\n",
    "        if gap < tol:\n",
    "            break\n",
    "        \n",
    "        support = np.where(x > 1e-9)[0]\n",
    "        if not len(support):\n",
    "            x.fill(0.0); x[s_idx] = 1.0\n",
    "            history.append(obj_func(x, A, **obj_kwargs))\n",
    "            continue\n",
    "            \n",
    "        v_idx = support[np.argmin(grad[support])]\n",
    "        \n",
    "        if s_idx == v_idx:\n",
    "            d = s_vertex - x\n",
    "            gamma_max = 1.0\n",
    "        else:\n",
    "            v_vertex = np.zeros(n); v_vertex[v_idx] = 1.0\n",
    "            d = s_vertex - v_vertex\n",
    "            gamma_max = x[v_idx]\n",
    "        \n",
    "        gamma = step_func(obj_func, x, d, A, grad, gamma_max, **obj_kwargs)\n",
    "        \n",
    "        if s_idx == v_idx:\n",
    "            x = (1.0 - gamma) * x + gamma * s_vertex\n",
    "        else:\n",
    "            x[s_idx] += gamma\n",
    "            x[v_idx] -= gamma\n",
    "        \n",
    "        x[x<1e-10] = 0.0\n",
    "        if np.sum(x) > 1e-9:\n",
    "            x /= np.sum(x)\n",
    "            \n",
    "        current_f = obj_func(x, A, **obj_kwargs)\n",
    "        history.append(current_f)\n",
    "        if current_f > f_best:\n",
    "            f_best = current_f\n",
    "            x_best = np.copy(x)\n",
    "\n",
    "    return x_best, history\n",
    "\n",
    "def away_step_frank_wolfe(A, reg_type, max_iter, tol, step_func):\n",
    "    \"\"\"Implementazione Corretta dell'Away-Step Frank-Wolfe.\"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    x0_idx = np.random.randint(n)\n",
    "    x = np.zeros(n)\n",
    "    x[x0_idx] = 1.0\n",
    "    \n",
    "    if reg_type == 'l2':\n",
    "        obj_func = f_l2\n",
    "        grad_func = grad_l2\n",
    "        obj_kwargs = {'alpha': 0.5}\n",
    "    else:\n",
    "        obj_func = f_l0\n",
    "        grad_func = grad_l0\n",
    "        obj_kwargs = {'alpha': 0.07, 'beta': 5}\n",
    "        \n",
    "    history = [obj_func(x, A, **obj_kwargs)]\n",
    "    x_best = np.copy(x)\n",
    "    f_best = history[0]\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        grad = grad_func(x, A, **obj_kwargs)\n",
    "        s_vertex, s_idx = lmo(-grad)\n",
    "        d_fw = s_vertex - x\n",
    "        gap_fw = np.dot(grad, d_fw)\n",
    "        if gap_fw < tol: break\n",
    "        \n",
    "        support = np.where(x > 1e-9)[0]\n",
    "        if not len(support):\n",
    "            x.fill(0.0); x[s_idx] = 1.0\n",
    "            history.append(obj_func(x, A, **obj_kwargs))\n",
    "            continue\n",
    "            \n",
    "        v_idx = support[np.argmin(grad[support])]\n",
    "        v_vertex = np.zeros(n); v_vertex[v_idx] = 1.0\n",
    "        d_away = x - v_vertex\n",
    "        progress_away = np.dot(grad, d_away)\n",
    "        \n",
    "        if gap_fw >= progress_away:\n",
    "            d = d_fw\n",
    "            gamma_max = 1.0\n",
    "        else:\n",
    "            d = d_away\n",
    "            alpha_v = x[v_idx]\n",
    "            gamma_max = alpha_v / (1.0 - alpha_v) if abs(1.0 - alpha_v) > 1e-9 else float('inf')\n",
    "        \n",
    "        gamma = step_func(obj_func, x, d, A, grad, gamma_max, **obj_kwargs)\n",
    "        \n",
    "        # L'aggiornamento per AFW Ã¨ x_new = x + gamma*d\n",
    "        x = x + gamma * d\n",
    "        \n",
    "        x[x<1e-10] = 0.0\n",
    "        if np.sum(x) > 1e-9:\n",
    "            x /= np.sum(x)\n",
    "            \n",
    "        current_f = obj_func(x, A, **obj_kwargs)\n",
    "        history.append(current_f)\n",
    "        if current_f > f_best:\n",
    "            f_best = current_f\n",
    "            x_best = np.copy(x)\n",
    "    \n",
    "    return x_best, history\n",
    "\n",
    "def projected_gradient(A, reg_type, max_iter, tol, step_func, lr_initial=0.001):\n",
    "    \"\"\"Implementazione del Gradiente Proiettato.\"\"\"\n",
    "    n = A.shape[0]; x = np.ones(n) / n\n",
    "    \n",
    "    if reg_type == 'l2':\n",
    "        obj_func = f_l2\n",
    "        grad_func = grad_l2\n",
    "        obj_kwargs = {'alpha': 0.5}\n",
    "    else:\n",
    "        obj_func = f_l0\n",
    "        grad_func = grad_l0\n",
    "        obj_kwargs = {'alpha': 0.07, 'beta': 5}\n",
    "        \n",
    "    history = [obj_func(x, A, **obj_kwargs)]\n",
    "    x_best = np.copy(x)\n",
    "    f_best = history[0]\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        grad = grad_func(x, A, **obj_kwargs)\n",
    "        \n",
    "        # Per PGD la direzione Ã¨ il gradiente. Il passo Armijo Ã¨ una scelta valida.\n",
    "        gamma = step_func(obj_func, x, grad, A, grad, lr_initial, **obj_kwargs)\n",
    "        \n",
    "        x_next = projection_simplex(x + gamma * grad)\n",
    "        if np.linalg.norm(x_next - x) < tol: break\n",
    "        x = x_next\n",
    "        \n",
    "        current_f = obj_func(x, A, **obj_kwargs)\n",
    "        history.append(current_f)\n",
    "        if current_f > f_best:\n",
    "            f_best = current_f\n",
    "            x_best = np.copy(x)\n",
    "            \n",
    "    return x_best, history\n",
    "\n",
    "# ======================================================================\n",
    "# --- BLOCCO DI ESECUZIONE PRINCIPALE ---\n",
    "# ======================================================================\n",
    "if __name__ == '__main__':\n",
    "    # Aggiorna con i percorsi corretti ai tuoi file .mtx\n",
    "    graph_files = [\n",
    "        \"./data/C250-9.mtx\",\n",
    "        \"./data/C500-9.mtx\",\n",
    "        \"./data/C2000-9.mtx\"\n",
    "    ]\n",
    "    reg_types = ['l2', 'l0']\n",
    "    algorithms = {\n",
    "        'FW': frank_wolfe,\n",
    "        'PFW': pairwise_frank_wolfe,\n",
    "        'AFW': away_step_frank_wolfe,\n",
    "        'PGD': projected_gradient\n",
    "    }\n",
    "    \n",
    "    # OPZIONI PER LO STEPSIZE: Ricerca Esatta (Optimal) vs. Armijo (Inesatta)\n",
    "    stepsize_options = [\n",
    "        {'label': 'Optimal Gamma (LS)', 'func': exact_line_search},\n",
    "        {'label': 'Armijo Rule', 'func': armijo_line_search}\n",
    "    ]\n",
    "\n",
    "    results_list = []\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        print(f\"\\n--- Elaborazione del file: {graph_file} ---\")\n",
    "        A = load_graph(graph_file)\n",
    "        if A is None: continue\n",
    "        print(f\"Grafo caricato con {A.shape[0]} nodi.\")\n",
    "        \n",
    "        # Il multi-start Ã¨ importante per i problemi non convessi\n",
    "        num_random_starts = 5 # Aumenta per un'analisi piÃ¹ robusta\n",
    "        \n",
    "        for step_info in stepsize_options:\n",
    "            step_func = step_info['func']\n",
    "            step_label = step_info['label']\n",
    "            \n",
    "            print(f\"--- Esecuzione con strategia di passo: {step_label} ---\")\n",
    "            for reg_type in reg_types:\n",
    "                for algo_name, algo_func in algorithms.items():\n",
    "                    print(f\"Esecuzione: {algo_name} - Reg: {reg_type}\")\n",
    "                    \n",
    "                    best_clique_size_for_algo = 0\n",
    "                    best_x_for_algo = None\n",
    "                    total_runtime = 0\n",
    "                    \n",
    "                    # FW e PGD partono dal centro, non necessitano di multi-start in questo setup\n",
    "                    starts_to_run = num_random_starts if algo_name in ['PFW', 'AFW'] else 1\n",
    "                    \n",
    "                    for i in range(starts_to_run):\n",
    "                        try:\n",
    "                            start_time = time.perf_counter()\n",
    "                            \n",
    "                            x_best, history = algo_func(A=A, reg_type=reg_type, max_iter=100, tol=1e-5, step_func=step_func)\n",
    "                            \n",
    "                            runtime = time.perf_counter() - start_time\n",
    "                            total_runtime += runtime\n",
    "                            \n",
    "                            clique = extract_clique(x_best, A)\n",
    "                            if len(clique) > best_clique_size_for_algo:\n",
    "                                if validate_clique(clique, A):\n",
    "                                    best_clique_size_for_algo = len(clique)\n",
    "                                    best_x_for_algo = x_best # Salva il vettore soluzione migliore\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"ERRORE durante l'esecuzione di {algo_name}: {e}\")\n",
    "\n",
    "                    # Salva il risultato migliore del multi-start\n",
    "                    if best_x_for_algo is not None:\n",
    "                         clique = extract_clique(best_x_for_algo, A)\n",
    "                         is_valid = validate_clique(clique, A)\n",
    "                         results_list.append({\n",
    "                            'Graph File': graph_file.split('/')[-1], 'Algorithm': algo_name, 'Reg': reg_type,\n",
    "                            'Stepsize': step_label, 'Clique Size': len(clique), 'Valid': is_valid,\n",
    "                            'Runtime (s)': total_runtime / starts_to_run,\n",
    "                         })\n",
    "                    else: # Se nessun run ha prodotto una soluzione valida\n",
    "                         results_list.append({\n",
    "                            'Graph File': graph_file.split('/')[-1], 'Algorithm': algo_name, 'Reg': reg_type,\n",
    "                            'Stepsize': step_label, 'Clique Size': 0, 'Valid': False,\n",
    "                            'Runtime (s)': total_runtime / starts_to_run,\n",
    "                         })\n",
    "\n",
    "\n",
    "    # Stampa riepilogo tabellare\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"                              RIEPILOGO FINALE DEI RISULTATI                              \")\n",
    "    print(\"=\"*85)\n",
    "    if results_list:\n",
    "        df_results = pd.DataFrame(results_list)\n",
    "        print(df_results.to_string())\n",
    "    else:\n",
    "        print(\"Nessun risultato da visualizzare.\")\n",
    "    print(\"=\"*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0238c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
